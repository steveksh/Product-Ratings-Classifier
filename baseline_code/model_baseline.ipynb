{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remarks \n",
    "\n",
    "* BERT tokenizer had 768 word vectors, which can be a bit much. But the APIs for the tokenizer is very easy to use.\n",
    "    *  Embeddings are also a bit sensitive to puncutations and stopwords. \n",
    "    * You can try tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\") and print(tokenizer.tokenize(\"This is an example of the bert tokenizer\"))\n",
    "    * You should see that it tries to parse all elements in the sentence. \n",
    "* We can use Word2Vec but it may take a while to download the pretrained weights. The implementations are a bit harder \n",
    "    * Also word2vec is sensitive to unseen words. it will throw an exception when you try to vectorize unseen words. Therefore, we need to have a function to process them.\n",
    "    * For example, in the paper I believe it used \"UNK\" or simply \"pass\" / torch.zeros()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import sys\n",
    "import pandas as pd \n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# loading file \n",
    "with open('../components/review_embedding.pkl', 'rb') as handle:\n",
    "    x = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(x)\n",
    "df = pd.concat([df.loc[df.overall>=4].sample(frac=0.08,random_state=22),df.loc[df.overall<4].sample(frac=0.3,random_state=22)]).reset_index()\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df['embedding'], df['overall'], test_size= 0.33, random_state=22)\n",
    "\n",
    "# x_test, holdout_x_test, y_test, holdout_y_test = train_test_split(x_test, y_test, test_size= 0.2, random_state=22)\n",
    "\n",
    "# dataframes = [x_train, y_train, x_test, y_test, holdout_x_test, holdout_y_test]\n",
    "dataframes = [x_train, y_train, x_test, y_test]\n",
    "\n",
    "for i in dataframes:\n",
    "    i.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0    9487\n",
       "3.0    8528\n",
       "4.0    4335\n",
       "1.0    4311\n",
       "2.0    4018\n",
       "Name: overall, dtype: int64"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.overall.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewDataSet(Dataset):\n",
    "    def __init__(self, reviews, ratings):\n",
    "        self.reviews = reviews \n",
    "        self.ratings = ratings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ratings)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        reviews = self.reviews[index]\n",
    "        ratings = self.ratings[index]\n",
    "        \n",
    "        # pytorch starts at 0 just like python \n",
    "        return reviews, int(ratings)-1\n",
    "\n",
    "batch_size = 128\n",
    "train_set = ReviewDataSet(reviews=x_train, ratings=y_train)\n",
    "valid_set = ReviewDataSet(reviews=x_test, ratings=y_test)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=0)\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of reviews: torch.Size([128, 768])\n",
      "Type of reviews: torch.float32\n",
      "Size of label: torch.Size([128, 768])\n",
      "Type of label: torch.float32\n"
     ]
    }
   ],
   "source": [
    "for ratings, labels in train_loader:\n",
    "    print('Size of reviews:', ratings.size())  \n",
    "    print('Type of reviews:', ratings.dtype)   # float32\n",
    "    print('Size of label:', ratings.size())  # batch_size\n",
    "    print('Type of label:', ratings.dtype)   # int64(long)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    # def __init__(self):\n",
    "    def __init__(self, dw, h1, A):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.h1 = h1\n",
    "        self.Wa = nn.Parameter(torch.Tensor(dw, self.h1),requires_grad = True)\n",
    "        self.ba = nn.Parameter(torch.Tensor(self.h1),requires_grad = True)\n",
    "\n",
    "        ## Initialize weights with a uniform distribution between -0.01 and 0.01\n",
    "        nn.init.uniform_(self.Wa, -0.01, 0.01)\n",
    "\n",
    "        ## Initialize bias with zeros \n",
    "        nn.init.zeros_(self.ba) # zeros  \n",
    "        \n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(self.h1, 256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(256, 5),\n",
    "\n",
    "            # Probabilistic Output (link function)\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    # utils \n",
    "    def torch_device(self):\n",
    "        return \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    ## Multi-head word attention layer - not working \n",
    "    # def attention_layer(self, x):\n",
    "    #      multihead_attn = nn.MultiheadAttention(768, self.h1).to(\"cuda\")\n",
    "    #      x, _ = multihead_attn(x,x,x)\n",
    "    #      return x\n",
    "\n",
    "    def aspect_layers(self, x):\n",
    "        # X = f(X*W + b) where f() is the activation function according to the research paper \n",
    "        Xu_a = torch.matmul(x, self.Wa) + self.ba\n",
    "        Xu_a = F.leaky_relu(Xu_a)\n",
    "        return Xu_a\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.aspect_layers(x)\n",
    "        # x = self.attention_layer(x)\n",
    "\n",
    "        pred_ratings = self.fc_layers(x)\n",
    "        return pred_ratings\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [00:02<00:00, 80.44it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 001/005 ] loss = 1.50604, acc = 48.44720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [00:00<00:00, 145.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 001/005 ] loss = 1.45728, acc = 57.63750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [00:01<00:00, 133.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 002/005 ] loss = 1.43187, acc = 59.16149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [00:00<00:00, 189.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 002/005 ] loss = 1.42187, acc = 60.16250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [00:01<00:00, 155.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 003/005 ] loss = 1.41145, acc = 61.65217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [00:00<00:00, 212.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 003/005 ] loss = 1.41151, acc = 60.93750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [00:01<00:00, 153.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 004/005 ] loss = 1.40609, acc = 62.60870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [00:00<00:00, 216.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 004/005 ] loss = 1.40454, acc = 61.16250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [00:01<00:00, 157.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 005/005 ] loss = 1.39949, acc = 63.31677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [00:00<00:00, 229.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 005/005 ] loss = 1.41456, acc = 60.31250\n",
      "did not increase valid_acc. break counter now at 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Aspect-Sentiment Transformation Layer Parameters\n",
    "h1 = 50  # Hidden dimension\n",
    "dw = 768  # embedding dimension\n",
    "A = 3  # Number of aspects\n",
    "\n",
    "n_epochs = 5\n",
    "lr = 0.001\n",
    "break_counter = 0\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "best = float('inf')  # Initialize with a large value\n",
    "path = './model.pth'\n",
    "\n",
    "device = \"cuda\" \n",
    "model = Classifier(dw, h1, A).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(n_epochs): \n",
    "\n",
    "    # training mode\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    train_accs = []\n",
    "\n",
    "    for batch in tqdm(train_loader):\n",
    "        reviews, rating = batch \n",
    "\n",
    "        logits = model(reviews.to(device))\n",
    "        loss = criterion(logits, rating.to(device))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        pred = torch.argmax(logits, dim=1)\n",
    "        acc = sum(pred==rating.to(device)).item()\n",
    "\n",
    "        # acc = (logits.argmax(dim=-1) == rating.float().to(device)).float().mean()\n",
    "\n",
    "        # Record the loss and accuracy.\n",
    "        train_loss.append(loss.item())\n",
    "        train_accs.append(acc)\n",
    "    \n",
    "    # The average loss and accuracy of the training set is the average of the recorded values.\n",
    "    train_loss = sum(train_loss) / len(train_loss)\n",
    "    train_acc = sum(train_accs) / len(train_accs)\n",
    "\n",
    "    print(f\"[ Train | {epoch + 1:03d}/{n_epochs:03d} ] loss = {train_loss:.5f}, acc = {train_acc:.5f}\")\n",
    "\n",
    "    # ---------- Validation ----------\n",
    "    # Make sure the model is in eval mode so that some modules like dropout are disabled and work normally.\n",
    "    model.eval()\n",
    "\n",
    "    # These are used to record information in validation.\n",
    "    valid_loss = []\n",
    "    valid_accs = []\n",
    "\n",
    "    # Iterate the validation set by batches.\n",
    "    for batch in tqdm(valid_loader):\n",
    "\n",
    "        reviews, rating = batch \n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(reviews.to(device))\n",
    "\n",
    "        # We can still compute the loss (but not the gradient).\n",
    "        loss = criterion(logits, rating.to(device))\n",
    "\n",
    "        # Compute the accuracy for current batch.\n",
    "        pred = torch.argmax(logits, dim=1)\n",
    "        acc = sum(pred==rating.to(device)).item()\n",
    "\n",
    "        # Record the loss and accuracy.\n",
    "        valid_loss.append(loss.item())\n",
    "        valid_accs.append(acc)\n",
    "\n",
    "    # The average loss and accuracy for entire validation set is the average of the recorded values.\n",
    "    valid_loss = sum(valid_loss) / len(valid_loss)\n",
    "    valid_acc = sum(valid_accs) / len(valid_accs)\n",
    "    \n",
    "    print(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}\")\n",
    "\n",
    "    if valid_loss < best:\n",
    "        torch.save(model.state_dict(), path)\n",
    "        best = valid_loss\n",
    "        break_counter = 0\n",
    "    else:\n",
    "        break_counter += 1\n",
    "        print(f'did not increase valid_acc. break counter now at {break_counter}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try out on the test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3], dtype=int64)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Some modules like Dropout or BatchNorm affect if the model is in training mode.\n",
    "# model.eval()\n",
    "# # unsqueeze turns 1d tensor into 2d\n",
    "# embedding = torch.unsqueeze(torch.tensor(holdout_x_test[1]), 0).to(device)\n",
    "# # adding 1 because we initially -1 from all ratings\n",
    "# torch.argmax(model(embedding), dim=1).cpu().numpy()+1 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
